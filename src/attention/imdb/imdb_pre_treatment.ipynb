{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a the pretreatment of the IMDB csv to tokenize the text and save it in a parquet file.\n",
    "The CSV can be downloaded from [https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/GRZ/gramba/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/eleves-b/2021/remi.grzeczkowicz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "\n",
    "# Download stopwords if not already available\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load the csv file and show the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          1\n",
      "1  A wonderful little production. <br /><br />The...          1\n",
      "2  I thought this was a wonderful way to spend ti...          1\n",
      "3  Basically there's a family where a little boy ...          0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('IMDB_Dataset.csv')\n",
    "data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "#show the first 5 rows of the data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Remove stop words, html tags, punctuation, numbers, and lowercase the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "3  Basically there's a family where a little boy ...   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  one reviewers mentioned watching oz episode yo...  \n",
      "1  wonderful little production filming technique ...  \n",
      "2  thought wonderful way spend time hot summer we...  \n",
      "3  basically theres family little boy jake thinks...  \n",
      "4  petter matteis love time money visually stunni...  \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply function to the dataframe\n",
    "data['cleaned_review'] = data['review'].apply(clean_text)\n",
    "\n",
    "print(data[['review', 'cleaned_review']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Tokenize the text using bert tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "3  Basically there's a family where a little boy ...   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "\n",
      "                                      cleaned_review  \\\n",
      "0  one reviewers mentioned watching oz episode yo...   \n",
      "1  wonderful little production filming technique ...   \n",
      "2  thought wonderful way spend time hot summer we...   \n",
      "3  basically theres family little boy jake thinks...   \n",
      "4  petter matteis love time money visually stunni...   \n",
      "\n",
      "                                         bert_tokens  \\\n",
      "0  [one, reviewers, mentioned, watching, oz, epis...   \n",
      "1  [wonderful, little, production, filming, techn...   \n",
      "2  [thought, wonderful, way, spend, time, hot, su...   \n",
      "3  [basically, there, ##s, family, little, boy, j...   \n",
      "4  [pet, ##ter, matt, ##eis, love, time, money, v...   \n",
      "\n",
      "                                      bert_token_ids  \n",
      "0  [101, 2028, 15814, 3855, 3666, 11472, 2792, 20...  \n",
      "1  [101, 6919, 2210, 2537, 7467, 6028, 14477, 475...  \n",
      "2  [101, 2245, 6919, 2126, 5247, 2051, 2980, 2621...  \n",
      "3  [101, 10468, 2045, 2015, 2155, 2210, 2879, 518...  \n",
      "4  [101, 9004, 3334, 4717, 17580, 2293, 2051, 276...  \n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "data['bert_tokens'] = data['cleaned_review'].apply(lambda x: tokenizer.tokenize(x))\n",
    "data['bert_token_ids'] = data['cleaned_review'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "print(data[['review', 'cleaned_review', 'bert_tokens', 'bert_token_ids']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Split the data into train test and validation sets. Store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad the sequences to the same length of max length\n",
    "max_len = max(data['bert_token_ids'].apply(len))\n",
    "#extend all the sequences to the max length using padding token\n",
    "bart_token_list = data['bert_token_ids'].tolist()\n",
    "for i in range(len(bart_token_list)):\n",
    "    bart_token_list[i] = bart_token_list[i] + [0]*(max_len-len(bart_token_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully processed and saved for model training!\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensors\n",
    "input_ids = torch.tensor(bart_token_list)\n",
    "labels = torch.tensor(data['sentiment'].tolist())\n",
    "\n",
    "# Split dataset into Train (80%), Validation (10%), Test (10%)\n",
    "train_inputs, temp_inputs, train_labels, temp_labels = train_test_split(input_ids, labels, test_size=0.2, random_state=42)\n",
    "val_inputs, test_inputs, val_labels, test_labels = train_test_split(temp_inputs, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save tensors for later use\n",
    "torch.save(train_inputs, 'train_inputs.pt')\n",
    "torch.save(train_labels, 'train_labels.pt')\n",
    "torch.save(val_inputs, 'val_inputs.pt')\n",
    "torch.save(val_labels, 'val_labels.pt')\n",
    "torch.save(test_inputs, 'test_inputs.pt')\n",
    "torch.save(test_labels, 'test_labels.pt')\n",
    "\n",
    "print(\"Data successfully processed and saved for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 : to load the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-e81c5c18b0f2>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_inputs = torch.load('train_inputs.pt')\n",
      "<ipython-input-13-e81c5c18b0f2>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_labels = torch.load('train_labels.pt')\n",
      "<ipython-input-13-e81c5c18b0f2>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_inputs = torch.load('val_inputs.pt')\n",
      "<ipython-input-13-e81c5c18b0f2>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_labels = torch.load('val_labels.pt')\n",
      "<ipython-input-13-e81c5c18b0f2>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_inputs = torch.load('test_inputs.pt')\n",
      "<ipython-input-13-e81c5c18b0f2>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_labels = torch.load('test_labels.pt')\n"
     ]
    }
   ],
   "source": [
    "train_inputs = torch.load('train_inputs.pt')\n",
    "train_labels = torch.load('train_labels.pt')\n",
    "val_inputs = torch.load('val_inputs.pt')\n",
    "val_labels = torch.load('val_labels.pt')\n",
    "test_inputs = torch.load('test_inputs.pt')\n",
    "test_labels = torch.load('test_labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 2008, 2015,  ...,    0,    0,    0])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "#print a sample of the data\n",
    "print(train_inputs[0])\n",
    "print(train_labels[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
