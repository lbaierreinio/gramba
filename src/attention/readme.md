In this folder we implemented different types of linear attention mechanisms.

## Linear Self Attention
The linformer is implemented in a python package available [here](https://github.com/tatp22/linformer-pytorch). It can be downloaded using the following command:

```pip install linformer-pytorch```

It comes from the paper [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/pdf/2006.04768)

An exemple of how to use it is given in the file `linformer.py`.

## Windowed Self Attention
The windowed attention is implemented in the file `windowedAttention.py`. It comes from [this repo](https://github.com/allenai/longformer/tree/74523f7f2897b3a5ffcdb537f67a03f83aa1affb).
